# YOLOv8 Inference Benchmark: Python vs C++

This repository contains a performance comparison of YOLOv8 inference implementations using **Python (Ultralytics)** and **C++ (ONNX Runtime)**.

My goal was to analyze the overhead introduced by the Python interpreter, Global Interpreter Lock (GIL), Garbage Collector and Pytorch eager mode compared to a raw C++ .onnx implementation in a real-time computer vision context.


## ğŸ“‚ Project Structure
<pre>
â”œâ”€â”€ YoloCpp/                      # C++ implementation
â”‚   â”œâ”€â”€ src/
â”‚   |    â”œâ”€â”€ main.cpp             # Entry point, async loop, metrics
â”‚   |    â”œâ”€â”€ YoloDetector.h
â”‚   |    â””â”€â”€ YoloDetector.cpp
â”‚   â””â”€â”€ CMakeLists.txt            # Build configuration
â”‚
â”œâ”€â”€ YoloPython/                   # Python implementation
â”‚   â”œâ”€â”€ src/
â”‚   |    â”œâ”€â”€ fps_counter.py       # Separate fps counter on a single image (inference only)
â”‚   |    â”œâ”€â”€ model_export.py      # Script for exporting the model to .onnx
â”‚   |    â””â”€â”€ web_inference.py     # Benchmark script with threading
â”‚   â””â”€â”€ requirements.txt          # Python dependencies
â”‚
â””â”€â”€ README.md
</pre>


## ğŸ“Š Benchmark Results

In this project I measure not just FPS, but also:
- **Inference Latency:** Pure model computation time.
- **System Latency (End-to-End):** Time from frame capture to visualization.
- **Jitter (standard deviation):** Stability of the processing pipeline.

Tests were conducted on a webcam feed (640x480). The models were configured with identical confidence thresholds (0.5).

| Metric | Python (Ultralytics) | C++ (ONNX Runtime) |
| :--- | :--- | :--- |
| **Inference Time (Avg)** | ~13.1 ms | ~10.6 ms |
| **Inference Jitter (Std)** | ~2.8 ms | ~1.1 ms |
| **End-to-End Latency (Avg)** | ~21.0 ms | ~15.6 ms |
| **End-to-End Jitter (Std)** | ~7.5 ms | ~2.0 ms |
| **Real FPS** | ~48 FPS | ~64 FPS |
| **RAM utilization** | ~980 MB | ~700 MB |

**Observation:** While C++ slightly (~25% faster) outperforms Python in pure GPU inference time (+ pre/postprocessing CPU operations) and overall throughput (real FPS), the difference in system stability (Jitter) is significant due to the lack of GC overhead and faster pre/post-processing. It is critical for edge devices.


## ğŸ“ Methodology
**1. Warmup:** A dummy tensor is passed through the network to initialize CUDA contexts and allocate memory buffers.

**2. Threaded Capture:** A separate thread constantly grabs the latest frame from the camera buffer.

**3. Measurement:**
- Inference Time: Run() method execution time.
- Frame Time: Time between loop iterations (includes visualization imshow overhead).
